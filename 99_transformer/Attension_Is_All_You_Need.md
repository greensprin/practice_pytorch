# Abstract
これまでの機械翻訳モデルはエンコーダー、デコーダーを含む、複雑な再帰処理や畳み込みニューラルネットワーク(CNN)が支配的だった。		

ベストな性能を出したモデルもアテンション機構を通してエンコーダーデコーダーが接続されている		

我々は、再帰処理や畳み込み処理を必要としない、アテンション機構をベースとしたシンプルな新しいネットワーク(Transformer)を提案する		

2つの機械翻訳実験で、このモデルは並列化と大幅なトレーニング時間の削減という点で優れていることを示す		

いろいろな翻訳大会でいい感じの成績出したよ（省略）		

我々はTransformerを大量の学習データと制限された学習データの両方を用いて英語の構文解析に適用することで、他タスクにも汎用的に適用できることを示す		

※2つの機械翻訳実験 = 大量データでの学習 と 制限されたデータでの学習のこと？その結果を比較するということ？		

## 要約
これまでのモデルよりシンプルにしつつ、性能を出せることを、2つの実験を用いて示しますということみたい。		

# Introduction
既存の再帰型ニューラルネットワークとして以下が出てきている		

Recurrent neural network (LSTM = long short term memory, GRNN = gated recurrent neural network)		
		
機械翻訳のような連続したデータを扱うモデルにおいて、LSTM, GRNNは確立された方法です。		

様々な検討により、これらのモデルは限界を広げてきた		
		
再帰モデルは通常、入出力列の象徴的な場所に沿って計算を行います。		

計算を進めるためポジションを整列させ、これらのモデルは隠れ状態hを生成する、前の隠れ状態ht-1と入力tに対して。		

この順番に処理しないといけないということは、並列化を妨げます		

また、メモリの制約がバッチサイズの制限になります。		

後者についてはいろいろな取り組みにより改善されたが、		

前者は依然として問題は残っている		
		
アテンション機構は様々なタスクでモデルの重要な一部となった		

入出力の文章の長さに依存しないモデル作成を可能にしたことで		

このアテンション機構は、いくつかの場合を除いて、再帰ネットワークと組み合わせて使われている		

我々の提案するTransformerでは、再帰処理を使わず、入出力の関係を知るために全体的にアテンション機構を使う		

これによりTransformerは処理を大幅に並列化できるようにし、8個のP100GPUでの12時間ほどの学習により、翻訳において新しい段階の性能を出している		

## 要約
従来の再帰型のNNは、順番に処理しないといけないため並列化が難しく、またメモリの制約によりバッチサイズが制限されていたことも並列化を難しくしていた		

メモリについては近年改善されてきたが、処理を順番にやらないといけないことは依然として問題となっている		

Transformerでは再帰処理を使わず、アテンション機構を用いることで大幅な並列化を可能にし、8個のGPUで12時間学習するだけで、翻訳においてこれまでにない性能を出すことを可能とした		

# Background
計算を減らす目的の物として、Extended Neural GPU, ByteNet, ConvS2sが作られたが、これらは畳み込み層NNを基本歳て作成されている

入力から出力までの距離が長くなるほど、ConvS2Sは背系的に、ByteNetは対数的に計算量が増えてしまう

Transformerは、固定の計算量で計算が可能で、我々はMulti-Head Attensionでそれらに対抗している

## 要約
これまでのモデルも計算量を減らそうと頑張ったけど、あまりよくいかなかった

Transformerは、Multi-Head Attensionを使うことで、層が深くなっても計算量を減らせる

# Model Architecture
競争力のあるモデルはエンコーダーデコーダー構造を持っている

各段階で、モデルは自己回帰を行う。前に作られた信号を次に信号を作る時に加算する。

Transformerはこのような構造に従います。

全結合層はエンコーダー、デコーダー両方に存在しています。

## 要約
図の通りのアーキテクチャになってますという説明

# Encoder and Decoder Stacks

## Encoder
エンコーダーは6つの同一の層で作られます

1つの層は2つのサブ層を持ち、

1つ目はMulti-Head Self-Attention Mecanismで、

2つ目はシンプルな全結合層です。

2つのサブ層の後には正規化の層があります。

それぞれのサブ層の出力は以下のような感じ

```python
    # Encoder sub Layer
    LayerNrom(x + Sublayer(x))
```

これらの残渣接続層を促進する為、このサブ層の出力は512次元となる (埋め込みそうと同様に)

## Decoder
デコーダーも同様に6つの同一層から作られる

デコーダーではエンコーダーの2層に加えて、3つ目のサブ層を追加する

これはエンコーダーの出力に対してMulti-Head Attentionを実行する

エンコーダーと同様、残渣接続を各サブ層に対して行う。その後、正規化を行う。

また、最初のアテンション機構も修正する。 (何かを防ぐためらしいがよくわからず)

このマスク処理は、出力の埋め込みが1つずつオフセットされていることと相まって、

i番目の位置の予測はi番目より小さい位置の吉の出力二のみ依存することを保証する

### 要約
ここも図の通り。ただし、Decoderのマスク処理についてはよくわからなかった..

# Attension
アテンション機構は、query と key-valueのペアをoutputにマッピングすることで説明される。

この時、query, key, value, outputはすべてvectorである

outputはvalue * weightの合計で計算される。

各value毎に割り当てられているweightは、計算される queryの互換関数によって 対応するkeyで (よくわからず)

## Scaled Dot-Product Attention
我々は特定のアテンション機構を"Scaled Dot-Product Attention"と呼ぶ

入力はqueryとkeyで構成されており、次元はdkとする

また、valueの次元はdvとする

我々はqueryのdot積(dot products)をすべてのキーで計算する

その後sqrt(dk)でそれぞれを除算し、sofmax関数を適用する。valueの重みを得るために。