# Abstract
これまでの機械翻訳モデルはエンコーダー、デコーダーを含む、複雑な再帰処理や畳み込みニューラルネットワーク(CNN)が支配的だった。		

ベストな性能を出したモデルもアテンション機構を通してエンコーダーデコーダーが接続されている		

我々は、再帰処理や畳み込み処理を必要としない、アテンション機構をベースとしたシンプルな新しいネットワーク(Transformer)を提案する		

2つの機械翻訳実験で、このモデルは並列化と大幅なトレーニング時間の削減という点で優れていることを示す		

いろいろな翻訳大会でいい感じの成績出したよ（省略）		

我々はTransformerを大量の学習データと制限された学習データの両方を用いて英語の構文解析に適用することで、他タスクにも汎用的に適用できることを示す		

※2つの機械翻訳実験 = 大量データでの学習 と 制限されたデータでの学習のこと？その結果を比較するということ？		

## 要約
これまでのモデルよりシンプルにしつつ、性能を出せることを、2つの実験を用いて示しますということみたい。		

# Introduction
既存の再帰型ニューラルネットワークとして以下が出てきている		

Recurrent neural network (LSTM = long short term memory, GRNN = gated recurrent neural network)		
		
機械翻訳のような連続したデータを扱うモデルにおいて、LSTM, GRNNは確立された方法です。		

様々な検討により、これらのモデルは限界を広げてきた		
		
再帰モデルは通常、入出力列の象徴的な場所に沿って計算を行います。		

計算を進めるためポジションを整列させ、これらのモデルは隠れ状態hを生成する、前の隠れ状態ht-1と入力tに対して。		

この順番に処理しないといけないということは、並列化を妨げます		

また、メモリの制約がバッチサイズの制限になります。		

後者についてはいろいろな取り組みにより改善されたが、		

前者は依然として問題は残っている		
		
アテンション機構は様々なタスクでモデルの重要な一部となった		

入出力の文章の長さに依存しないモデル作成を可能にしたことで		

このアテンション機構は、いくつかの場合を除いて、再帰ネットワークと組み合わせて使われている		

我々の提案するTransformerでは、再帰処理を使わず、入出力の関係を知るために全体的にアテンション機構を使う		

これによりTransformerは処理を大幅に並列化できるようにし、8個のP100GPUでの12時間ほどの学習により、翻訳において新しい段階の性能を出している		

## 要約
従来の再帰型のNNは、順番に処理しないといけないため並列化が難しく、またメモリの制約によりバッチサイズが制限されていたことも並列化を難しくしていた		

メモリについては近年改善されてきたが、処理を順番にやらないといけないことは依然として問題となっている		

Transformerでは再帰処理を使わず、アテンション機構を用いることで大幅な並列化を可能にし、8個のGPUで12時間学習するだけで、翻訳においてこれまでにない性能を出すことを可能とした		

# Background
計算を減らす目的の物として、Extended Neural GPU, ByteNet, ConvS2sが作られたが、これらは畳み込み層NNを基本歳て作成されている

入力から出力までの距離が長くなるほど、ConvS2Sは背系的に、ByteNetは対数的に計算量が増えてしまう

Transformerは、固定の計算量で計算が可能で、我々はMulti-Head Attensionでそれらに対抗している

## 要約
これまでのモデルも計算量を減らそうと頑張ったけど、あまりよくいかなかった

Transformerは、Multi-Head Attensionを使うことで、層が深くなっても計算量を減らせる

# Model Architecture